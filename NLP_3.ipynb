{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KalusaniLaxman/nlp_lab/blob/main/NLP_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbh4qFe3NITZ",
        "outputId": "f05ecbd9-d05a-4eea-bbcf-e0af53ccb4ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "# Removed nltk.tokenize import\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Removed nltk.download(\"punkt\") and nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"/content/archive (3).zip\") # Uncommented to load the dataset\n",
        "# Please upload your dataset and update the path above, or ensure the file is at the specified location.\n",
        "\n",
        "# The rest of the code will not run until `df` is defined. Once you have the file loaded, you can uncomment the line above.\n",
        "# print(\"Dataset shape:\", df.shape)\n",
        "# print(\"Columns:\", df.columns.tolist())\n",
        "# print(\"\\nClass counts:\")\n",
        "# print(df[\"target\"].value_counts())\n",
        "\n",
        "# df = df.dropna(subset=[\"text\"])\n",
        "# print(\"\\nRandom 3 disaster tweets:\")\n",
        "# print(df[df[\"target\"]==1][\"text\"].sample(3, random_state=42).tolist())\n",
        "# print(\"\\nRandom 3 irrelevant tweets:\")\n",
        "# print(df[df[\"target\"]==0][\"text\"].sample(3, random_state=42).tolist())\n",
        "\n",
        "# plt.figure(figsize=(6,4))\n",
        "# sns.countplot(x=\"target\", data=df)\n",
        "# plt.xticks([0,1], [\"Irrelevant (0)\",\"Disaster (1)\"])\n",
        "# plt.title(\"Class distribution\")\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# stop_words = set(stopwords.words(\"english\"))\n",
        "# ps = PorterStemmer()\n",
        "# wnl = WordNetLemmatizer()\n",
        "\n",
        "# def remove_urls_mentions_hashtags(text):\n",
        "#     text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \" \", text, flags=re.IGNORECASE)\n",
        "#     text = re.sub(r\"@\\w+\", \" \", text)\n",
        "#     text = re.sub(r\"#\\w+\", \" \", text)\n",
        "#     return text\n",
        "\n",
        "# emoji_pattern = re.compile(\n",
        "#     \"[\"\n",
        "#     \"\\U0001F600-\\U0001F64F\"\n",
        "#     \"\\U0001F300-\\U0001F5FF\"\n",
        "#     \"\\U0001F680-\\U0001F6FF\"\n",
        "#     \"\\U0001F1E0-\\U0001F1FF\"\n",
        "#     \"]+\", flags=re.UNICODE)\n",
        "\n",
        "# def remove_emojis(text):\n",
        "#     return emoji_pattern.sub(r\" \", text)\n",
        "\n",
        "# def normalize_elongated(word):\n",
        "#     return re.sub(r'(.)\\1{2,}', r'\\1\\1', word)\n",
        "\n",
        "# def penn_to_wordnet_pos(tag):\n",
        "#     if tag.startswith(\"J\"):\n",
        "#         return wordnet.ADJ\n",
        "#     elif tag.startswith(\"V\"):\n",
        "#         return wordnet.VERB\n",
        "#     elif tag.startswith(\"N\"):\n",
        "#         return wordnet.NOUN\n",
        "#     elif tag.startswith(\"R\"):\n",
        "#         return wordnet.ADV\n",
        "#     else:\n",
        "#         return wordnet.NOUN\n",
        "\n",
        "# def preprocess(text):\n",
        "#     text = remove_urls_mentions_hashtags(text)\n",
        "#     text = remove_emojis(text)\n",
        "#     text = text.lower()\n",
        "#     # Use regex for tokenization\n",
        "#     tokens = re.findall(r'\\b\\w+\\b', text) # Using regex to find words\n",
        "#     tokens = [normalize_elongated(t) for t in tokens]\n",
        "#     tokens = [t for t in tokens if t not in stop_words and t.isalpha()]\n",
        "#     return tokens\n",
        "\n",
        "# df[\"clean_tokens\"] = df[\"text\"].apply(preprocess)\n",
        "# df[\"clean_text\"] = df[\"clean_tokens\"].apply(lambda toks: \" \".join(toks))\n",
        "\n",
        "# sample5 = df[\"text\"].head(5).tolist()\n",
        "# stemmed = []\n",
        "# # lemmatized = [] # Commented out lemmatized list\n",
        "# for sent in sample5:\n",
        "#     toks = preprocess(sent)\n",
        "#     stemmed.append(\" \".join([ps.stem(t) for t in toks]))\n",
        "#     # pos_tag still relies on NLTK resources, might fail again\n",
        "#     # If it fails, we might need to remove pos_tag and lemmatization or find an alternative\n",
        "#     # pos_tags = pos_tag(toks) # Commented out pos_tag call\n",
        "#     # lem = [] # Commented out lem list\n",
        "#     # for w, tag in pos_tags: # Commented out loop\n",
        "#     #     pos = penn_to_wordnet_pos(tag) # Commented out pos conversion\n",
        "#     #     lem.append(wnl.lemmatize(w, pos=pos)) # Commented out lemmatization\n",
        "#     # lemmatized.append(\" \".join(lem)) # Commented out appending to lemmatized\n",
        "\n",
        "# # compare_df = pd.DataFrame({\"Original\": sample5, \"Stemmed\": stemmed, \"Lemmatized\": lemmatized}) # Commented out DataFrame creation\n",
        "# # print(\"\\nOriginal -> Stemmed -> Lemmatized (first 5 tweets):\") # Commented out print statement\n",
        "# # print(compare_df.to_string(index=False)) # Commented out print statement\n",
        "\n",
        "# disaster_texts = df[df[\"target\"]==1][\"clean_text\"]\n",
        "# all_disaster_joined = \" \".join(disaster_texts.tolist())\n",
        "# wordcloud = WordCloud(width=800, height=400, background_color=\"white\", max_words=200).generate(all_disaster_joined)\n",
        "# plt.figure(figsize=(12,6))\n",
        "# plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "# plt.axis(\"off\")\n",
        "# plt.title(\"Word Cloud - Disaster Tweets\")\n",
        "# plt.show()\n",
        "\n",
        "# bigram_vectorizer = CountVectorizer(ngram_range=(2,2), max_features=200, stop_words=\"english\")\n",
        "# bigram_counts = bigram_vectorizer.fit_transform(disaster_texts)\n",
        "# bigram_sums = np.asarray(bigram_counts.sum(axis=0)).ravel()\n",
        "# bigrams = bigram_vectorizer.get_feature_names_out()\n",
        "# bigram_freq = sorted(list(zip(bigrams, bigram_sums)), key=lambda x: x[1], reverse=True)[:10]\n",
        "# bigram_df = pd.DataFrame(bigram_freq, columns=[\"bigram\",\"count\"])\n",
        "# plt.figure(figsize=(8,5))\n",
        "# sns.barplot(x=\"count\", y=\"bigram\", data=bigram_df)\n",
        "# plt.title(\"Top 10 Bigrams in Disaster Tweets\")\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# df[\"token_len\"] = df[\"clean_tokens\"].apply(len)\n",
        "# plt.figure(figsize=(10,5))\n",
        "# sns.histplot(df[df[\"target\"]==1][\"token_len\"], label=\"Disaster\", color=\"r\", kde=False, bins=30)\n",
        "# sns.histplot(df[df[\"target\"]==0][\"token_len\"], label=\"Irrelevant\", color=\"b\", kde=False, bins=30)\n",
        "# plt.legend()\n",
        "# plt.xlabel(\"Token length\")\n",
        "# plt.title(\"Tweet length distribution by class\")\n",
        "# plt.show()\n",
        "\n",
        "# print(\"\\nMean token lengths:\")\n",
        "# print(df.groupby(\"target\")[\"token_len\"].mean())\n",
        "\n",
        "# tfidf = TfidfVectorizer(max_features=7000, ngram_range=(1,2))\n",
        "# X = tfidf.fit_transform(df[\"clean_text\"])\n",
        "# y = df[\"target\"].values\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# lr = LogisticRegression(max_iter=1000, random_state=42)\n",
        "# rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "\n",
        "# lr.fit(X_train, y_train)\n",
        "# rf.fit(X_train, y_train)\n",
        "\n",
        "# models = {\"LogisticRegression\": lr, \"RandomForest\": rf}\n",
        "# results = {}\n",
        "# for name, model in models.items():\n",
        "#     y_pred = model.predict(X_test)\n",
        "#     acc = accuracy_score(y_test, y_pred)\n",
        "#     prec = precision_score(y_test, y_pred)\n",
        "#     rec = recall_score(y_test, y_pred)\n",
        "#     f1 = f1_score(y_test, y_pred)\n",
        "#     results[name] = {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
        "#     print(f\"\\nModel: {name}\")\n",
        "#     print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "# best_model_name = max(results.items(), key=lambda x: x[1][\"f1\"])[0]\n",
        "# best_model = models[best_model_name]\n",
        "# print(\"\\nBest model by F1:\", best_model_name)\n",
        "\n",
        "# y_pred_best = best_model.predict(X_test)\n",
        "# cm = confusion_matrix(y_test, y_pred_best)\n",
        "# plt.figure(figsize=(5,4))\n",
        "# sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Pred 0\",\"Pred 1\"], yticklabels=[\"True 0\",\"True 1\"])\n",
        "# plt.title(f\"Confusion Matrix - {best_model_name}\")\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    }
  ]
}