{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeNPNJRceDm3pFzVbpH2gy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KalusaniLaxman/nlp_lab/blob/main/proj%20_expo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHCtF10LDoFE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "909e1cf8"
      },
      "source": [
        "# Task\n",
        "Build, train, and evaluate a combined CNN and RNN (RCNN/C-LSTM) model for sentiment analysis using the `Tweets.csv` dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c759972"
      },
      "source": [
        "## Load and Inspect Data\n",
        "\n",
        "### Subtask:\n",
        "Load the `Tweets.csv` dataset into a pandas DataFrame and perform an initial inspection to understand its structure, columns, and identify the text and target variables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb1cb6d8"
      },
      "source": [
        "**Reasoning**:\n",
        "I will load the `Tweets.csv` dataset into a pandas DataFrame, display its head, information, and descriptive statistics to understand its structure, columns, and variable types, which is essential for identifying text and target variables.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d10dbd8a",
        "outputId": "764f6811-f112-4921-ba68-21c8620d5064"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the Tweets.csv file into a pandas DataFrame named df\n",
        "df = pd.read_csv('/content/Tweets.csv')\n",
        "\n",
        "print(\"First 5 rows of the DataFrame:\")\n",
        "# Display the first 5 rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nDataFrame Info:\")\n",
        "# Print a summary of the DataFrame\n",
        "df.info()\n",
        "\n",
        "print(\"\\nDescriptive statistics of the DataFrame:\")\n",
        "# Generate descriptive statistics of the numerical columns\n",
        "print(df.describe())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of the DataFrame:\n",
            "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
            "0  570306133677760513           neutral                        1.0000   \n",
            "1  570301130888122368          positive                        0.3486   \n",
            "2  570301083672813571           neutral                        0.6837   \n",
            "3  570301031407624196          negative                        1.0000   \n",
            "4  570300817074462722          negative                        1.0000   \n",
            "\n",
            "  negativereason  negativereason_confidence         airline  \\\n",
            "0            NaN                        NaN  Virgin America   \n",
            "1            NaN                     0.0000  Virgin America   \n",
            "2            NaN                        NaN  Virgin America   \n",
            "3     Bad Flight                     0.7033  Virgin America   \n",
            "4     Can't Tell                     1.0000  Virgin America   \n",
            "\n",
            "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
            "0                    NaN     cairdin                 NaN              0   \n",
            "1                    NaN    jnardino                 NaN              0   \n",
            "2                    NaN  yvonnalynn                 NaN              0   \n",
            "3                    NaN    jnardino                 NaN              0   \n",
            "4                    NaN    jnardino                 NaN              0   \n",
            "\n",
            "                                                text tweet_coord  \\\n",
            "0                @VirginAmerica What @dhepburn said.         NaN   \n",
            "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
            "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
            "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
            "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
            "\n",
            "               tweet_created tweet_location               user_timezone  \n",
            "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
            "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
            "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
            "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
            "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  \n",
            "\n",
            "DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 14640 entries, 0 to 14639\n",
            "Data columns (total 15 columns):\n",
            " #   Column                        Non-Null Count  Dtype  \n",
            "---  ------                        --------------  -----  \n",
            " 0   tweet_id                      14640 non-null  int64  \n",
            " 1   airline_sentiment             14640 non-null  object \n",
            " 2   airline_sentiment_confidence  14640 non-null  float64\n",
            " 3   negativereason                9178 non-null   object \n",
            " 4   negativereason_confidence     10522 non-null  float64\n",
            " 5   airline                       14640 non-null  object \n",
            " 6   airline_sentiment_gold        40 non-null     object \n",
            " 7   name                          14640 non-null  object \n",
            " 8   negativereason_gold           32 non-null     object \n",
            " 9   retweet_count                 14640 non-null  int64  \n",
            " 10  text                          14640 non-null  object \n",
            " 11  tweet_coord                   1019 non-null   object \n",
            " 12  tweet_created                 14640 non-null  object \n",
            " 13  tweet_location                9907 non-null   object \n",
            " 14  user_timezone                 9820 non-null   object \n",
            "dtypes: float64(2), int64(2), object(11)\n",
            "memory usage: 1.7+ MB\n",
            "\n",
            "Descriptive statistics of the DataFrame:\n",
            "           tweet_id  airline_sentiment_confidence  negativereason_confidence  \\\n",
            "count  1.464000e+04                  14640.000000               10522.000000   \n",
            "mean   5.692184e+17                      0.900169                   0.638298   \n",
            "std    7.791112e+14                      0.162830                   0.330440   \n",
            "min    5.675883e+17                      0.335000                   0.000000   \n",
            "25%    5.685592e+17                      0.692300                   0.360600   \n",
            "50%    5.694779e+17                      1.000000                   0.670600   \n",
            "75%    5.698905e+17                      1.000000                   1.000000   \n",
            "max    5.703106e+17                      1.000000                   1.000000   \n",
            "\n",
            "       retweet_count  \n",
            "count   14640.000000  \n",
            "mean        0.082650  \n",
            "std         0.745778  \n",
            "min         0.000000  \n",
            "25%         0.000000  \n",
            "50%         0.000000  \n",
            "75%         0.000000  \n",
            "max        44.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52072471"
      },
      "source": [
        "## Text Preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Clean and preprocess the text data from the tweets, including tokenization, converting text to sequences, and padding sequences to a uniform length, and encode the target variable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26f847f9"
      },
      "source": [
        "**Reasoning**:\n",
        "I will extract the text and target variables, encode the target variable using `LabelEncoder`, and then preprocess the text data by tokenizing, converting to sequences, and padding them to a uniform length using `Tokenizer` and `pad_sequences` from Keras, following the instructions provided.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "000ed9b5",
        "outputId": "5d431890-aa9a-4531-f96a-183fe1360c91"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 1. Extract the 'text' column for the input features and the 'airline_sentiment' column for the target variable\n",
        "X = df['text']\n",
        "y = df['airline_sentiment']\n",
        "\n",
        "print(f\"Original X shape: {X.shape}\")\n",
        "print(f\"Original y shape: {y.shape}\")\n",
        "\n",
        "# 2. Initialize a LabelEncoder and fit it to the 'airline_sentiment' column\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(y)\n",
        "\n",
        "print(f\"Encoded labels shape: {encoded_labels.shape}\")\n",
        "print(f\"Original labels: {y.head().tolist()}\")\n",
        "print(f\"Encoded labels: {encoded_labels[:5].tolist()}\")\n",
        "print(f\"Classes: {label_encoder.classes_}\")\n",
        "\n",
        "# 3. Initialize a Tokenizer\n",
        "num_words = 10000 # You can adjust this number based on your vocabulary size\n",
        "oov_token = \"<unk>\"\n",
        "tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
        "\n",
        "# 4. Fit the tokenizer on the extracted text data\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "print(f\"\\nTokenizer vocabulary size: {len(tokenizer.word_index)}\")\n",
        "print(\"Top 10 words in vocabulary:\", list(tokenizer.word_index.items())[:10])\n",
        "\n",
        "# 5. Convert the text data into sequences of integers\n",
        "text_sequences = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "print(f\"\\nFirst 5 original texts: {X.head().tolist()}\")\n",
        "print(f\"First 5 text sequences: {text_sequences[:5]}\")\n",
        "\n",
        "# 6. Determine a suitable maximum sequence length for padding\n",
        "max_sequence_length = max(len(seq) for seq in text_sequences)\n",
        "print(f\"\\nMaximum sequence length: {max_sequence_length}\")\n",
        "\n",
        "# 7. Pad the text_sequences\n",
        "padded_sequences = pad_sequences(text_sequences, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "print(f\"\\nPadded sequences shape: {padded_sequences.shape}\")\n",
        "print(f\"First padded sequence: {padded_sequences[0].tolist()}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original X shape: (14640,)\n",
            "Original y shape: (14640,)\n",
            "Encoded labels shape: (14640,)\n",
            "Original labels: ['neutral', 'positive', 'neutral', 'negative', 'negative']\n",
            "Encoded labels: [1, 2, 1, 0, 0]\n",
            "Classes: ['negative' 'neutral' 'positive']\n",
            "\n",
            "Tokenizer vocabulary size: 15769\n",
            "Top 10 words in vocabulary: [('<unk>', 1), ('to', 2), ('the', 3), ('i', 4), ('a', 5), ('united', 6), ('you', 7), ('for', 8), ('flight', 9), ('on', 10)]\n",
            "\n",
            "First 5 original texts: ['@VirginAmerica What @dhepburn said.', \"@VirginAmerica plus you've added commercials to the experience... tacky.\", \"@VirginAmerica I didn't today... Must mean I need to take another trip!\", '@VirginAmerica it\\'s really aggressive to blast obnoxious \"entertainment\" in your guests\\' faces &amp; they have little recourse', \"@VirginAmerica and it's a really big bad thing about it\"]\n",
            "First 5 text sequences: [[82, 63, 6687, 227], [82, 559, 591, 1160, 2537, 2, 3, 202, 6688], [82, 4, 208, 103, 806, 592, 4, 77, 2, 157, 151, 194], [82, 90, 137, 3793, 2, 4707, 4708, 1010, 16, 22, 6689, 3794, 60, 58, 23, 504, 2799], [82, 11, 90, 5, 137, 477, 215, 488, 85, 21]]\n",
            "\n",
            "Maximum sequence length: 36\n",
            "\n",
            "Padded sequences shape: (14640, 36)\n",
            "First padded sequence: [82, 63, 6687, 227, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baf336d9"
      },
      "source": [
        "## Build Text Embedding Layer\n",
        "\n",
        "### Subtask:\n",
        "Create an embedding layer for the preprocessed text data. This layer will convert numerical sequences into dense vector representations that the CNN and RNN layers can process effectively.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebc6d3c2"
      },
      "source": [
        "**Reasoning**:\n",
        "I will import the `Embedding` layer, define the `embedding_dim`, and instantiate the embedding layer using the previously calculated `num_words` and `max_sequence_length`, as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87103d6c",
        "outputId": "aa84b522-5419-40b4-91f6-c3c062ae46cf"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "# 1. Define the embedding dimension\n",
        "embedding_dim = 100  # You can experiment with different dimensions\n",
        "\n",
        "# 2. Create the Embedding layer\n",
        "# The input_dim is num_words (vocabulary size)\n",
        "# The output_dim is embedding_dim\n",
        "# The input_length is max_sequence_length (length of padded sequences)\n",
        "embedding_layer = Embedding(input_dim=num_words,\n",
        "                            output_dim=embedding_dim,\n",
        "                            input_length=max_sequence_length,\n",
        "                            name=\"embedding_layer\")\n",
        "\n",
        "print(f\"Embedding layer created with input_dim={num_words}, output_dim={embedding_dim}, input_length={max_sequence_length}\")\n",
        "print(f\"Embedding layer output shape: (None, {max_sequence_length}, {embedding_dim})\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding layer created with input_dim=10000, output_dim=100, input_length=36\n",
            "Embedding layer output shape: (None, 36, 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0755c57d"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code generated a UserWarning because the `input_length` argument for the `Embedding` layer is deprecated. I will remove this argument as suggested by the warning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a8122e1",
        "outputId": "cd3e0105-81d3-4401-c0b3-7228e24402ed"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "# 1. Define the embedding dimension\n",
        "embedding_dim = 100  # You can experiment with different dimensions\n",
        "\n",
        "# 2. Create the Embedding layer\n",
        "# The input_dim is num_words (vocabulary size)\n",
        "# The output_dim is embedding_dim\n",
        "embedding_layer = Embedding(input_dim=num_words,\n",
        "                            output_dim=embedding_dim,\n",
        "                            name=\"embedding_layer\")\n",
        "\n",
        "print(f\"Embedding layer created with input_dim={num_words}, output_dim={embedding_dim}\")\n",
        "print(f\"Embedding layer output shape: (None, None, {embedding_dim})\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding layer created with input_dim=10000, output_dim=100\n",
            "Embedding layer output shape: (None, None, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce396976"
      },
      "source": [
        "## Define RCNN/C-LSTM Model Architecture\n",
        "\n",
        "### Subtask:\n",
        "Construct the combined CNN and RNN (LSTM) model architecture, defining convolutional layers, pooling layers, LSTM layers, and combining them in an RCNN or C-LSTM fashion.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c41e2524"
      },
      "source": [
        "**Reasoning**:\n",
        "I will import the necessary Keras layers and `Model` class, then define the input layer, apply the embedding, create separate CNN and LSTM branches, concatenate their outputs, add dense layers, and finally instantiate the model as described in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "ef4d933a",
        "outputId": "3005cceb-6a72-4061-ed1f-919d4b9b7453"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, GlobalMaxPooling1D, LSTM, Dense, concatenate\n",
        "\n",
        "# Get the number of unique classes for the output layer\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# 1. Define the input layer\n",
        "# The input shape is max_sequence_length because padded_sequences has this length\n",
        "input_layer = Input(shape=(max_sequence_length,), name=\"input_layer\")\n",
        "\n",
        "# 2. Apply the previously created embedding_layer to the input\n",
        "embedded_sequences = embedding_layer(input_layer)\n",
        "\n",
        "# 3. Create a convolutional branch (CNN)\n",
        "cov_branch = Conv1D(filters=128, kernel_size=5, activation='relu', name=\"conv_layer\")(embedded_sequences)\n",
        "cov_branch = GlobalMaxPooling1D(name=\"global_max_pooling_layer\")(cov_branch)\n",
        "\n",
        "# 4. Create an RNN branch (LSTM)\n",
        "rnn_branch = LSTM(128, name=\"lstm_layer\")(embedded_sequences)\n",
        "\n",
        "# 5. Concatenate the outputs of the CNN branch and the RNN branch\n",
        "merged_output = concatenate([cov_branch, rnn_branch], name=\"concatenate_layer\")\n",
        "\n",
        "# 6. Add one or more Dense layers for feature learning\n",
        "dense_layer = Dense(64, activation='relu', name=\"dense_layer_1\")(merged_output)\n",
        "\n",
        "# 7. Add the final output Dense layer\n",
        "output_layer = Dense(num_classes, activation='softmax', name=\"output_layer\")(dense_layer)\n",
        "\n",
        "# 8. Instantiate the Model\n",
        "model = Model(inputs=input_layer, outputs=output_layer, name=\"rcnn_clstm_model\")\n",
        "\n",
        "# Print the model summary to verify the architecture\n",
        "model.summary()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"rcnn_clstm_model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"rcnn_clstm_model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_layer     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m100\u001b[0m)   │  \u001b[38;5;34m1,000,000\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv_layer (\u001b[38;5;33mConv1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │     \u001b[38;5;34m64,128\u001b[0m │ embedding_layer[\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_layer (\u001b[38;5;33mLSTM\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │    \u001b[38;5;34m117,248\u001b[0m │ embedding_layer[\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_layer   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_max_pooli… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ lstm_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m16,448\u001b[0m │ concatenate_laye… │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ output_layer        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │        \u001b[38;5;34m195\u001b[0m │ dense_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_layer     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">64,128</span> │ embedding_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │ embedding_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_layer   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_max_pooli… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ lstm_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │ concatenate_laye… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ output_layer        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │ dense_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,198,019\u001b[0m (4.57 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,198,019</span> (4.57 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,198,019\u001b[0m (4.57 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,198,019</span> (4.57 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5a9455a"
      },
      "source": [
        "**Reasoning**:\n",
        "The model architecture has been defined. The next step is to compile the model by specifying the optimizer, loss function, and metrics, which is crucial before training the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "061124d4",
        "outputId": "2cf15831-c7ed-41aa-eabc-bfbef90b8bae"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# 9. Compile the model\n",
        "# Use Adam optimizer and sparse_categorical_crossentropy loss function (since labels are integer encoded)\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"Model compiled successfully with Adam optimizer, sparse_categorical_crossentropy loss, and accuracy metric.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model compiled successfully with Adam optimizer, sparse_categorical_crossentropy loss, and accuracy metric.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "839509a2"
      },
      "source": [
        "## Compile and Train Model\n",
        "\n",
        "### Subtask:\n",
        "Compile the defined deep learning model with an appropriate optimizer, loss function, and metrics. Then, train the model using the preprocessed text data and corresponding target labels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9078e45"
      },
      "source": [
        "**Reasoning**:\n",
        "I will split the preprocessed data into training and testing sets using `train_test_split` to prepare for model training. Then, I will train the model using the `fit` method with the training data and validate it using the testing data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3511e67",
        "outputId": "7cc165c2-fd71-48b9-b529-2c2721bf50df"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, encoded_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "# 2. Train the model\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_data=(X_test, y_test))\n",
        "\n",
        "print(\"Model training complete.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (11712, 36)\n",
            "X_test shape: (2928, 36)\n",
            "y_train shape: (11712,)\n",
            "y_test shape: (2928,)\n",
            "Epoch 1/10\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 85ms/step - accuracy: 0.6783 - loss: 0.7602 - val_accuracy: 0.8036 - val_loss: 0.5000\n",
            "Epoch 2/10\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 83ms/step - accuracy: 0.8535 - loss: 0.3802 - val_accuracy: 0.8077 - val_loss: 0.4831\n",
            "Epoch 3/10\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 86ms/step - accuracy: 0.9274 - loss: 0.2062 - val_accuracy: 0.8016 - val_loss: 0.5417\n",
            "Epoch 4/10\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 85ms/step - accuracy: 0.9774 - loss: 0.0901 - val_accuracy: 0.7964 - val_loss: 0.7226\n",
            "Epoch 5/10\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 83ms/step - accuracy: 0.9888 - loss: 0.0472 - val_accuracy: 0.7917 - val_loss: 0.8510\n",
            "Epoch 6/10\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 84ms/step - accuracy: 0.9929 - loss: 0.0294 - val_accuracy: 0.7879 - val_loss: 0.8665\n",
            "Epoch 7/10\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 87ms/step - accuracy: 0.9952 - loss: 0.0209 - val_accuracy: 0.7780 - val_loss: 0.9470\n",
            "Epoch 8/10\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 82ms/step - accuracy: 0.9962 - loss: 0.0207 - val_accuracy: 0.7886 - val_loss: 0.9962\n",
            "Epoch 9/10\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 82ms/step - accuracy: 0.9958 - loss: 0.0181 - val_accuracy: 0.7859 - val_loss: 0.9982\n",
            "Epoch 10/10\n",
            "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 84ms/step - accuracy: 0.9951 - loss: 0.0156 - val_accuracy: 0.7763 - val_loss: 1.0611\n",
            "Model training complete.\n"
          ]
        }
      ]
    }
  ]
}